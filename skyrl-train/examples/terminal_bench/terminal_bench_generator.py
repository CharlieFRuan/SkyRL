import asyncio
import logging
from dataclasses import dataclass
from typing import List, Optional, Dict, Any, Set
from loguru import logger
from uuid import uuid4
from skyrl_train.generators.base import GeneratorInterface, GeneratorInput, GeneratorOutput, TrajectoryID
from skyrl_train.generators.utils import (
    get_rollout_metrics,
    get_response_ids_and_loss_mask_from_messages,
    extract_logprobs_from_rollout_details,
)
from skyrl_train.inference_engines.inference_engine_client import InferenceEngineClient
from skyrl_train.inference_engines.base import ConversationType
from skyrl_train.utils.reward_shaping import shape_reward_from_output
from omegaconf import DictConfig
from pathlib import Path

# Harbor orchestrator and trial imports
from harbor.orchestrators.queue import QueueOrchestrator
from harbor.models.trial.config import TrialConfig
from harbor.models.trial.result import TrialResult

# Schema-driven Harbor config mapping
from examples.terminal_bench.harbor_config import HarborConfigBuilder

# Maximum restart attempts for orchestrator recovery
MAX_ORCHESTRATOR_RESTART_ATTEMPTS = 3

@dataclass
class TerminalBenchAgentOutput:
    response_ids: List[int]
    reward: float
    stop_reason: str
    loss_mask: List[int]
    prompt_ids: List[int]
    trajectory_id: TrajectoryID
    summarization_count: Optional[int] = None
    rollout_logprobs: Optional[List[float]] = None
    # For RLOO-N: True = exclude from baseline (infrastructure failure)
    # False = include in baseline (agent failure or success)
    exclude_from_baseline: bool = False
    # Store the exception type for debugging/logging
    exception_type: Optional[str] = None

class TerminalBenchGenerator(GeneratorInterface):
    def __init__(
        self,
        generator_cfg: DictConfig,
        terminal_bench_cfg: DictConfig,
        inference_engine_client: InferenceEngineClient,
        tokenizer,
    ):
        """
        Args:
            generator_cfg: DictConfig object containing the generator configuration
            terminal_bench_cfg: DictConfig object containing the terminal bench configuration
            inference_engine_client: InferenceEngineClient object for interacting with the inference engines
            tokenizer: tokenizer object for encoding and decoding text
        """
        self.base_url = f"http://{generator_cfg.http_endpoint_host}:{generator_cfg.http_endpoint_port}"
        self.generator_cfg = generator_cfg
        self.tokenizer = tokenizer
        self.model_name = generator_cfg.model_name

        # Core terminal bench config
        self.trials_dir = terminal_bench_cfg.trials_dir

        # Schema-driven Harbor config builder
        # Automatically maps YAML fields to Harbor's TrialConfig with validation
        self._harbor_config_builder = HarborConfigBuilder(terminal_bench_cfg)

        # Configure Harbor log level (default WARNING to reduce noise)
        harbor_log_level = self._harbor_config_builder.get_log_level(default="WARNING")
        self._configure_harbor_logging(harbor_log_level)

        # Store model_info for external access (e.g., metrics)
        self.model_info = self._harbor_config_builder.model_info

        # Build retry config for QueueOrchestrator (handles backoff, exception filtering)
        self._retry_config = self._harbor_config_builder.build_retry_config()
        self._n_concurrent_trials = self._harbor_config_builder.get_n_concurrent_trials(
            default=16  # Reasonable default for parallel trial execution
        )

        # Reward shaping config (parses test output for partial credit)
        self._reward_shaping_config = self._harbor_config_builder.get_reward_shaping_config()

        # Error handling config (for RLOO-N advantage estimator)
        self._error_handling_config = self._harbor_config_builder.get_error_handling_config()

        logger.info(
            f"TerminalBenchGenerator initialized with HarborConfigBuilder. "
            f"Exposed fields: {list(self._harbor_config_builder._harbor_cfg.keys())}. "
            f"Retry config: max_retries={self._retry_config.max_retries}, "
            f"backoff={self._retry_config.min_wait_sec}-{self._retry_config.max_wait_sec}s. "
            f"Concurrent trials: {self._n_concurrent_trials}. "
            f"Reward shaping: enabled={self._reward_shaping_config.get('enable_reward_shaping', True)}, "
            f"shaper={self._reward_shaping_config.get('reward_shaper', 'pass_ratio')}. "
            f"Error classification: enabled={self._error_handling_config.get('enable_error_classification', False)}"
        )

        # Read custom chat template
        custom_chat_template_path = generator_cfg.engine_init_kwargs.get("custom_chat_template_chat_completion_path", None)
        if custom_chat_template_path:
            with open(custom_chat_template_path, "r") as f:
                self.custom_chat_template_content = f.read()
            logger.info(f"TerminalBenchGenerator initialized with custom chat template read from: {custom_chat_template_path}")
        else:
            self.custom_chat_template_content = None

        # Shared QueueOrchestrator state (initialized in startup())
        # This ensures all concurrent generate() calls share a single orchestrator
        # with a global n_concurrent_trials limit, rather than each worker creating
        # its own orchestrator (which would multiply the concurrency limit)
        self._orchestrator: Optional[QueueOrchestrator] = None
        self._orchestrator_lock: Optional[asyncio.Lock] = None  # Protects orchestrator lifecycle
        self._orchestrator_started: bool = False
        self._orchestrator_restart_count: int = 0

        # Eval session state (separate orchestrator for eval runs)
        # Each eval run gets a fresh orchestrator that is destroyed after the eval completes
        self._eval_orchestrator: Optional[QueueOrchestrator] = None
        self._eval_orchestrator_lock: Optional[asyncio.Lock] = None
        self._eval_session_active: bool = False
        self._eval_session_name: Optional[str] = None
        self._eval_trials_dir: Optional[str] = None

        # Eval-specific timeout (default 900s = 15 minutes)
        self._eval_timeout_override_sec = self._harbor_config_builder.get_eval_timeout_override_sec(default=900)

    def _configure_harbor_logging(self, level: str) -> None:
        """
        Configure Harbor's logging level.

        Args:
            level: Log level string (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        """
        log_level = getattr(logging, level.upper(), logging.WARNING)

        # Set level for Harbor's main logger and all child loggers
        harbor_loggers = [
            "harbor",
            "harbor.trial",
            "harbor.agents",
            "harbor.verifier",
            "harbor.orchestrators",
            "harbor.environments",
            "harbor.utils.logger",
        ]

        for logger_name in harbor_loggers:
            logging.getLogger(logger_name).setLevel(log_level)

        # Also set the root harbor logger
        logging.getLogger("harbor").setLevel(log_level)

        logger.info(f"Harbor logging level set to {level}")

    async def startup(self) -> None:
        """Initialize shared QueueOrchestrator for all generate() calls.

        This creates a single orchestrator that enforces the n_concurrent_trials
        limit globally across all async workers. Without this, each generate()
        call would create its own orchestrator, multiplying the concurrency limit.

        Called once by the trainer before the first generate() call.
        """
        self._orchestrator_lock = asyncio.Lock()
        await self._create_orchestrator()
        logger.info(
            f"TerminalBenchGenerator startup complete. "
            f"Shared orchestrator ready with n_concurrent_trials={self._n_concurrent_trials}"
        )

    async def _create_orchestrator(self) -> None:
        """Create and start a new QueueOrchestrator.

        Used for initial startup and for recovery after orchestrator failures.
        """
        self._orchestrator = QueueOrchestrator(
            trial_configs=[],  # We submit dynamically via submit_batch()
            n_concurrent_trials=self._n_concurrent_trials,
            metrics={},  # SkyRL handles its own metrics
            quiet=True,
            retry_config=self._retry_config,
        )
        await self._orchestrator.start()
        self._orchestrator_started = True
        logger.info(
            f"QueueOrchestrator created and started with "
            f"n_concurrent_trials={self._n_concurrent_trials}"
        )

    async def _restart_orchestrator(self) -> bool:
        """Restart the orchestrator after a failure.

        Uses locking to ensure only one restart happens at a time, even with
        concurrent generate() calls. Other callers wait for the restart to complete.

        Returns:
            True if restart succeeded, False if max attempts exceeded.
        """
        async with self._orchestrator_lock:
            # Check if another caller already restarted while we were waiting
            if self._orchestrator_started and self._orchestrator is not None:
                logger.info("Orchestrator already restarted by another caller")
                return True

            self._orchestrator_restart_count += 1
            if self._orchestrator_restart_count > MAX_ORCHESTRATOR_RESTART_ATTEMPTS:
                logger.error(
                    f"Max orchestrator restart attempts ({MAX_ORCHESTRATOR_RESTART_ATTEMPTS}) "
                    f"exceeded. Giving up."
                )
                return False

            logger.warning(
                f"Restarting QueueOrchestrator (attempt {self._orchestrator_restart_count}/"
                f"{MAX_ORCHESTRATOR_RESTART_ATTEMPTS})"
            )

            # Shutdown the failed orchestrator if it exists
            if self._orchestrator is not None:
                try:
                    await self._orchestrator.shutdown(wait=False)
                except Exception as e:
                    logger.warning(f"Error shutting down failed orchestrator: {e}")
                finally:
                    self._orchestrator = None
                    self._orchestrator_started = False

            # Create a new orchestrator
            try:
                await self._create_orchestrator()
                return True
            except Exception as e:
                logger.error(f"Failed to create new orchestrator: {e}")
                self._orchestrator_started = False
                return False

    async def shutdown(self) -> None:
        """Cleanup shared QueueOrchestrator.

        Called once by the trainer after the last generate() call.
        Safe to call multiple times (idempotent).
        """
        if self._orchestrator_lock is None:
            # startup() was never called
            return

        async with self._orchestrator_lock:
            if self._orchestrator is not None and self._orchestrator_started:
                try:
                    logger.info("Shutting down shared QueueOrchestrator...")
                    await self._orchestrator.shutdown(wait=True)
                    logger.info("QueueOrchestrator shutdown complete")
                except Exception as e:
                    logger.warning(f"Error during orchestrator shutdown: {e}")
                finally:
                    self._orchestrator_started = False
                    self._orchestrator = None

    async def start_eval_session(
        self,
        run_name: str,
        eval_step: int,
        val_set_name: Optional[str] = None,
    ) -> None:
        """Start a fresh eval session with its own QueueOrchestrator.

        Each eval run gets a dedicated orchestrator that is destroyed after eval completes.
        This ensures eval trials don't interfere with training trials and provides
        clean isolation for metrics and artifacts.

        Args:
            run_name: The job run name (from cfg.trainer.run_name).
            eval_step: The current global step (for unique naming).
            val_set_name: Optional name of the validation set being evaluated.
        """
        if self._eval_orchestrator_lock is None:
            self._eval_orchestrator_lock = asyncio.Lock()

        async with self._eval_orchestrator_lock:
            # Ensure any previous eval session is cleaned up
            if self._eval_session_active and self._eval_orchestrator is not None:
                logger.warning("Previous eval session still active, shutting it down first")
                try:
                    await self._eval_orchestrator.shutdown(wait=True)
                except Exception as e:
                    logger.warning(f"Error shutting down previous eval orchestrator: {e}")
                finally:
                    self._eval_orchestrator = None
                    self._eval_session_active = False

            # Build unique session name
            val_set_suffix = f"_{val_set_name}" if val_set_name else ""
            self._eval_session_name = f"{run_name}_eval{val_set_suffix}_step{eval_step}"

            # Create unique trials directory for this eval session
            if self.trials_dir:
                self._eval_trials_dir = str(Path(self.trials_dir) / "eval_sessions" / self._eval_session_name)
                Path(self._eval_trials_dir).mkdir(parents=True, exist_ok=True)
            else:
                self._eval_trials_dir = self.trials_dir

            logger.info(
                f"Starting eval session: {self._eval_session_name} "
                f"(timeout={self._eval_timeout_override_sec}s, trials_dir={self._eval_trials_dir})"
            )

            # Create fresh orchestrator for eval with eval-specific timeout
            self._eval_orchestrator = QueueOrchestrator(
                trial_configs=[],  # We submit dynamically via submit_batch()
                n_concurrent_trials=self._n_concurrent_trials,
                metrics={},  # SkyRL handles its own metrics
                quiet=True,
                retry_config=self._retry_config,
            )
            await self._eval_orchestrator.start()
            self._eval_session_active = True

            logger.info(
                f"Eval session {self._eval_session_name} started with fresh QueueOrchestrator "
                f"(n_concurrent_trials={self._n_concurrent_trials})"
            )

    async def stop_eval_session(self) -> None:
        """Stop the current eval session and destroy its orchestrator.

        Should be called after each evaluation run completes.
        Safe to call multiple times (idempotent).
        """
        if self._eval_orchestrator_lock is None:
            return

        async with self._eval_orchestrator_lock:
            if self._eval_orchestrator is not None and self._eval_session_active:
                session_name = self._eval_session_name or "unknown"
                try:
                    logger.info(f"Stopping eval session: {session_name}")
                    await self._eval_orchestrator.shutdown(wait=True)
                    logger.info(f"Eval session {session_name} orchestrator shutdown complete")
                except Exception as e:
                    logger.warning(f"Error during eval orchestrator shutdown: {e}")
                finally:
                    self._eval_orchestrator = None
                    self._eval_session_active = False
                    self._eval_session_name = None
                    self._eval_trials_dir = None

    def _get_active_orchestrator(self) -> Optional[QueueOrchestrator]:
        """Get the currently active orchestrator (eval or training).

        Returns:
            The eval orchestrator if an eval session is active, otherwise the training orchestrator.
        """
        if self._eval_session_active and self._eval_orchestrator is not None:
            return self._eval_orchestrator
        return self._orchestrator

    def _get_active_trials_dir(self) -> Optional[str]:
        """Get the trials directory for the currently active mode (eval or training).

        Returns:
            The eval trials directory if an eval session is active, otherwise the training trials_dir.
        """
        if self._eval_session_active and self._eval_trials_dir is not None:
            return self._eval_trials_dir
        return self.trials_dir

    def _get_active_timeout_override(self) -> Optional[int]:
        """Get the timeout override for the currently active mode (eval or training).

        Returns:
            The eval timeout if an eval session is active, otherwise None (use config default).
        """
        if self._eval_session_active:
            return self._eval_timeout_override_sec
        return None

    def _create_all_failed_output(
        self,
        trajectory_ids: List[TrajectoryID],
        exception_type: str = "OrchestratorFailure",
    ) -> GeneratorOutput:
        """Create a GeneratorOutput where all trajectories failed.

        Used when the orchestrator itself fails and cannot process any trials.
        All outputs are marked as infrastructure failures (excluded from baseline).
        """
        num_trials = len(trajectory_ids)
        return {
            "prompt_token_ids": [[0] for _ in range(num_trials)],
            "response_ids": [[0] for _ in range(num_trials)],
            "rewards": [0.0 for _ in range(num_trials)],
            "loss_masks": [[0] for _ in range(num_trials)],
            "stop_reasons": ["error" for _ in range(num_trials)],
            "rollout_metrics": {
                "generate/num_failed_instances": num_trials,
                "generate/num_failed_trajectories": num_trials,
                "generate/num_masked_trajectories": num_trials,
                f"generate/exception_{exception_type}": num_trials,
            },
            "rollout_logprobs": None,
            "exclude_from_baseline": [True for _ in range(num_trials)],  # Infrastructure failure
        }

    async def generate(self, input_batch: GeneratorInput) -> GeneratorOutput:
        """
        Generate rollouts for a batch of prompts using the active QueueOrchestrator.

        The active orchestrator (eval or training) handles:
        - Global concurrency control across all async workers
        - Retry logic with exponential backoff
        - Exception filtering (retry transient errors, skip permanent ones)

        During eval sessions (started via start_eval_session()), uses a dedicated
        eval orchestrator with its own trials directory and timeout settings.

        This method includes restart logic to recover from orchestrator failures
        without killing the entire training job.
        """
        num_trials = len(input_batch["prompts"])
        is_eval = self._eval_session_active
        mode_str = f"eval ({self._eval_session_name})" if is_eval else "training"
        logger.info(f"Starting batch generation for {num_trials} trials (mode={mode_str})")

        # Get active trials directory and timeout override
        active_trials_dir = self._get_active_trials_dir()
        timeout_override = self._get_active_timeout_override()

        # Build all TrialConfigs upfront
        trial_configs: List[TrialConfig] = []
        trajectory_ids: List[TrajectoryID] = []

        # Harbor expects hosted_vllm model names with exactly one '/'.
        # Convert HuggingFace-style "org/model" to just "model" for the alias.
        model_alias = self.model_name.split("/")[-1] if "/" in self.model_name else self.model_name

        for i in range(num_trials):
            prompt = input_batch["prompts"][i]
            trajectory_id = input_batch["trajectory_ids"][i]

            # Generate session_id for sticky routing to inference engines
            session_id = uuid4().hex

            trial_config = self._harbor_config_builder.build_trial_config(
                task_path=prompt,
                trials_dir=active_trials_dir,
                model_name=f"hosted_vllm/{model_alias}",
                api_base=f"{self.base_url}/v1",
                session_id=session_id,
                timeout_override_sec=timeout_override,
            )
            trial_configs.append(trial_config)
            trajectory_ids.append(trajectory_id)

        # Get the active orchestrator (eval or training)
        active_orchestrator = self._get_active_orchestrator()
        orchestrator_started = (
            self._eval_session_active if is_eval else self._orchestrator_started
        )

        # Check if orchestrator is available
        if not orchestrator_started or active_orchestrator is None:
            logger.error(
                "QueueOrchestrator not available. Was startup() called? "
                "Attempting emergency restart..."
            )
            restart_success = await self._restart_orchestrator()
            if not restart_success:
                logger.error("Emergency orchestrator restart failed. Returning all-failed output.")
                return self._create_all_failed_output(trajectory_ids, "OrchestratorNotStarted")

        # Submit trials to active orchestrator with restart logic on failure
        results: List[TrialResult | Exception] = []
        try:
            # Submit all trials and collect futures
            futures = await active_orchestrator.submit_batch(trial_configs)

            # Wait for all trials to complete
            # Note: return_exceptions=True ensures individual trial failures don't
            # bubble up as exceptions - they're returned as Exception objects in results
            results = await asyncio.gather(*futures, return_exceptions=True)

        except Exception as orchestrator_error:
            # Orchestrator-level failure (not individual trial failures)
            # This indicates something is wrong with the orchestrator itself
            logger.error(
                f"Orchestrator-level failure during batch submission/gather: "
                f"{type(orchestrator_error).__name__}: {orchestrator_error}"
            )

            # For eval sessions, we don't retry - just fail
            if is_eval:
                logger.error("Eval session orchestrator failed. Returning all-failed output.")
                return self._create_all_failed_output(
                    trajectory_ids,
                    f"EvalOrchestratorFailure_{type(orchestrator_error).__name__}"
                )

            # Attempt to restart the training orchestrator
            restart_success = await self._restart_orchestrator()
            if not restart_success:
                logger.error(
                    "Orchestrator restart failed. Returning all-failed output "
                    "to avoid killing training job."
                )
                return self._create_all_failed_output(
                    trajectory_ids,
                    f"OrchestratorFailure_{type(orchestrator_error).__name__}"
                )

            # Retry once with the fresh orchestrator
            try:
                logger.info(f"Retrying batch of {num_trials} trials with restarted orchestrator")
                active_orchestrator = self._get_active_orchestrator()  # Refresh reference
                futures = await active_orchestrator.submit_batch(trial_configs)
                results = await asyncio.gather(*futures, return_exceptions=True)
            except Exception as retry_error:
                logger.error(
                    f"Retry after orchestrator restart also failed: "
                    f"{type(retry_error).__name__}: {retry_error}"
                )
                return self._create_all_failed_output(
                    trajectory_ids,
                    f"OrchestratorRetryFailure_{type(retry_error).__name__}"
                )

        # Process results into TerminalBenchAgentOutput
        all_outputs: List[TerminalBenchAgentOutput] = []
        for i, result in enumerate(results):
            trajectory_id = trajectory_ids[i]
            output = self._process_trial_result(result, trajectory_id)
            all_outputs.append(output)

        # For a group of trajectories (n_samples_per_prompt trajectories for the same prompt):
        # - If error classification is DISABLED: if ANY trajectory fails, zero ALL trajectories in the group
        # - If error classification is ENABLED (RLOO-N mode):
        #   - Infrastructure failures (exclude_from_baseline=True): mark for exclusion from baseline
        #   - Agent failures (exclude_from_baseline=False): include in baseline with reward=0
        #   - If ALL trajectories in a group fail, they all get excluded from baseline
        enable_error_classification = self._error_handling_config.get("enable_error_classification", False)

        failed_instance_ids = set()
        num_failed_trajectories = 0  # per-trajectory, rather than per-instance
        num_masked_trajectories = 0  # trajectories excluded from baseline
        successful_outputs: List[TerminalBenchAgentOutput] = []  # only for metrics purpose

        # Track failure types per instance for RLOO-N
        instance_has_infra_failure: Dict[str, bool] = {}
        instance_has_agent_failure: Dict[str, bool] = {}

        for output in all_outputs:
            if output.stop_reason == "error":
                failed_instance_ids.add(output.trajectory_id.instance_id)
                num_failed_trajectories += 1
                if output.exclude_from_baseline:
                    num_masked_trajectories += 1
                    instance_has_infra_failure[output.trajectory_id.instance_id] = True
                else:
                    instance_has_agent_failure[output.trajectory_id.instance_id] = True

        if enable_error_classification:
            # RLOO-N mode: preserve exclude_from_baseline flags, don't cascade failures
            for output in all_outputs:
                if output.stop_reason == "error":
                    # Error outputs already have correct exclude_from_baseline set
                    output.response_ids = [0]
                    output.loss_mask = [0]
                    output.prompt_ids = [0]
                    output.reward = 0
                    output.rollout_logprobs = None  # Clear logprobs to match response_ids length
                else:
                    successful_outputs.append(output)
        else:
            # Legacy mode: if any trajectory fails, zero entire group
            for output in all_outputs:
                if output.trajectory_id.instance_id in failed_instance_ids:
                    output.response_ids = [0]
                    output.stop_reason = "error"
                    output.loss_mask = [0]
                    output.prompt_ids = [0]
                    output.reward = 0
                    output.rollout_logprobs = None  # Clear logprobs to match response_ids length
                    output.exclude_from_baseline = False  # Legacy: include in baseline
                else:
                    successful_outputs.append(output)

        # Calculate rollout metrics for successful outputs
        if len(successful_outputs) > 0:
            rollout_metrics = get_rollout_metrics(
                [output.response_ids for output in successful_outputs],
                [output.reward for output in successful_outputs],
            )
            rollout_metrics["generate/trajectories_summarized"] = sum(1 for output in successful_outputs if output.summarization_count > 0)
            rollout_metrics["generate/trajectories_truncated"] = sum(1 for output in successful_outputs if output.stop_reason == "length")
        else:
            rollout_metrics = {}
        rollout_metrics["generate/num_failed_instances"] = len(failed_instance_ids)
        rollout_metrics["generate/num_failed_trajectories"] = num_failed_trajectories
        rollout_metrics["generate/num_masked_trajectories"] = num_masked_trajectories

        # Log exception type breakdown for debugging
        exception_counts: Dict[str, int] = {}
        for output in all_outputs:
            if output.exception_type:
                exception_counts[output.exception_type] = exception_counts.get(output.exception_type, 0) + 1
        if exception_counts:
            logger.info(f"Exception breakdown: {exception_counts}")
            for exc_type, count in exception_counts.items():
                rollout_metrics[f"generate/exception_{exc_type}"] = count

        logger.info(
            f"Batch generation complete: {num_trials - num_failed_trajectories}/{num_trials} successful, "
            f"{len(failed_instance_ids)} failed instances, "
            f"{num_masked_trajectories} masked (excluded from baseline)"
        )

        # Collect rollout_logprobs if any outputs have them (required for TIS)
        # For zeroed/failed trajectories (response_ids=[0]), use [0.0] to match length
        has_any_logprobs = any(output.rollout_logprobs is not None for output in all_outputs)
        rollout_logprobs_list = None
        if has_any_logprobs:
            rollout_logprobs_list = []
            for output in all_outputs:
                if output.rollout_logprobs is not None:
                    rollout_logprobs_list.append(output.rollout_logprobs)
                else:
                    # For zeroed trajectories, logprobs must match response_ids length
                    # response_ids is [0] (length 1), so logprobs should be [0.0]
                    rollout_logprobs_list.append([0.0] * len(output.response_ids))

        generator_output: GeneratorOutput = {
            "prompt_token_ids": [output.prompt_ids for output in all_outputs],
            "response_ids": [output.response_ids for output in all_outputs],
            "rewards": [output.reward for output in all_outputs],
            "loss_masks": [output.loss_mask for output in all_outputs],
            "stop_reasons": [output.stop_reason for output in all_outputs],
            "rollout_metrics": rollout_metrics,
            "rollout_logprobs": rollout_logprobs_list,
            "exclude_from_baseline": [output.exclude_from_baseline for output in all_outputs],
        }

        return generator_output

    def _classify_exception(self, exception: Exception) -> tuple[bool, str]:
        """
        Classify an exception as infrastructure failure (mask) or agent failure (zero).

        Args:
            exception: The exception to classify.

        Returns:
            Tuple of (exclude_from_baseline, exception_type_name)
            - exclude_from_baseline=True: Infrastructure failure, exclude from RLOO-N baseline
            - exclude_from_baseline=False: Agent failure, include in baseline with reward=0
        """
        exception_type = type(exception).__name__

        # If error classification is disabled, treat all errors as agent failures
        if not self._error_handling_config.get("enable_error_classification", False):
            return False, exception_type

        mask_exceptions = self._error_handling_config.get("mask_exceptions", set())
        zero_exceptions = self._error_handling_config.get("zero_exceptions", set())
        default_treatment = self._error_handling_config.get("default_error_treatment", "zero")

        # Check if this exception type should be masked (excluded from baseline)
        if exception_type in mask_exceptions:
            logger.debug(f"Exception {exception_type} classified as MASK (infrastructure failure)")
            return True, exception_type

        # Check if this exception type should be zeroed (included in baseline)
        if exception_type in zero_exceptions:
            logger.debug(f"Exception {exception_type} classified as ZERO (agent failure)")
            return False, exception_type

        # Default treatment for unclassified exceptions
        exclude = (default_treatment == "mask")
        logger.debug(
            f"Exception {exception_type} not in config, using default treatment: "
            f"{'MASK' if exclude else 'ZERO'}"
        )
        return exclude, exception_type

    def _process_trial_result(
        self,
        result: TrialResult | Exception,
        trajectory_id: TrajectoryID,
    ) -> TerminalBenchAgentOutput:
        """
        Process a TrialResult from QueueOrchestrator into TerminalBenchAgentOutput.

        Args:
            result: TrialResult from Harbor or an Exception if the trial failed completely.
            trajectory_id: The trajectory ID for this trial.

        Returns:
            TerminalBenchAgentOutput with processed rollout data.
        """
        # Handle exceptions from the orchestrator
        if isinstance(result, Exception):
            exclude_from_baseline, exception_type = self._classify_exception(result)
            logger.warning(
                f"Trajectory {trajectory_id} failed with exception: {result} "
                f"(type={exception_type}, exclude_from_baseline={exclude_from_baseline})"
            )
            return TerminalBenchAgentOutput(
                response_ids=[0],
                reward=0,
                stop_reason="error",
                loss_mask=[0],
                prompt_ids=[0],
                trajectory_id=trajectory_id,
                exclude_from_baseline=exclude_from_baseline,
                exception_type=exception_type,
            )

        # Check for missing verifier result (trial ran but didn't produce valid output)
        if not result.verifier_result:
            # Try to get exception info from the result
            exception_info = getattr(result, "exception_info", None)
            exception_type = "UnknownError"
            exclude_from_baseline = False

            if exception_info:
                # Extract exception type from exception_info if available
                if hasattr(exception_info, "exception_type"):
                    exception_type = exception_info.exception_type
                elif hasattr(exception_info, "__class__"):
                    exception_type = type(exception_info).__name__

                # Create a mock exception to classify
                class MockException(Exception):
                    pass
                MockException.__name__ = exception_type
                exclude_from_baseline, _ = self._classify_exception(MockException())

            logger.warning(
                f"Trajectory {trajectory_id} failed: No verifier result. "
                f"Exception info: {exception_info} "
                f"(type={exception_type}, exclude_from_baseline={exclude_from_baseline})"
            )
            return TerminalBenchAgentOutput(
                response_ids=[0],
                reward=0,
                stop_reason="error",
                loss_mask=[0],
                prompt_ids=[0],
                trajectory_id=trajectory_id,
                exclude_from_baseline=exclude_from_baseline,
                exception_type=exception_type,
            )

        # Extract data from successful trial
        try:
            original_reward = result.verifier_result.rewards["reward"]
            chat_history = result.agent_result.metadata['all_messages']
            summarization_count = result.agent_result.metadata['summarization_count']
        except (KeyError, AttributeError, TypeError) as e:
            # Data extraction failure is typically an infrastructure issue
            exception_type = type(e).__name__
            exclude_from_baseline, _ = self._classify_exception(e)
            logger.warning(
                f"Trajectory {trajectory_id} failed: Could not extract results. "
                f"Error: {e}, Result: {result} "
                f"(type={exception_type}, exclude_from_baseline={exclude_from_baseline})"
            )
            return TerminalBenchAgentOutput(
                response_ids=[0],
                reward=0,
                stop_reason="error",
                loss_mask=[0],
                prompt_ids=[0],
                trajectory_id=trajectory_id,
                exclude_from_baseline=exclude_from_baseline,
                exception_type=exception_type,
            )

        # Apply reward shaping if enabled
        if self._reward_shaping_config.get("enable_reward_shaping", True):
            verifier_stdout = getattr(result.verifier_result, "stdout", None)
            reward = shape_reward_from_output(
                stdout=verifier_stdout,
                original_reward=original_reward,
                parser_name=self._reward_shaping_config.get("reward_parser"),
                shaper_name=self._reward_shaping_config.get("reward_shaper", "pass_ratio"),
                shaper_kwargs=self._reward_shaping_config.get("shaper_kwargs", {}),
                fallback_to_original=self._reward_shaping_config.get("reward_shaping_fallback", True),
            )
            if reward != original_reward:
                logger.debug(
                    f"Trajectory {trajectory_id}: reward shaped {original_reward:.3f} -> {reward:.3f}"
                )
        else:
            reward = original_reward

        # Validate chat history structure
        if not chat_history or len(chat_history) < 2 or chat_history[0]["role"] != "user":
            # Invalid chat history is typically an infrastructure/serialization issue
            logger.warning(
                f"Trajectory {trajectory_id} failed: Invalid chat history structure. "
                f"chat_history: {chat_history}"
            )
            return TerminalBenchAgentOutput(
                response_ids=[0],
                reward=0,
                stop_reason="error",
                loss_mask=[0],
                prompt_ids=[0],
                trajectory_id=trajectory_id,
                exclude_from_baseline=True,  # Infrastructure issue
                exception_type="InvalidChatHistory",
            )

        # Process successful trial
        # Use the first message as the prompt (assume no system messages)
        prompt = [chat_history[0]]
        prompt_ids = self.tokenizer.apply_chat_template(
            prompt,
            add_generation_prompt=False,
            tokenize=True,
            chat_template=self.custom_chat_template_content,
        )
        initial_prompt_length = len(prompt_ids)

        # Process response messages (everything after the first message)
        response_messages = chat_history[1:]

        # Extract per-turn logprobs from Harbor's rollout_details (required for TIS)
        rollout_details = getattr(result.agent_result, "rollout_details", None)
        assistant_logprobs = extract_logprobs_from_rollout_details(rollout_details)

        response_ids, loss_mask, rollout_logprobs = get_response_ids_and_loss_mask_from_messages(
            response_messages, self.tokenizer, assistant_logprobs, custom_chat_template=self.custom_chat_template_content
        )

        # Determine stop reason
        max_response_tokens = (
            self.generator_cfg.sampling_params.max_generate_length
            + self.generator_cfg.max_input_length
            - initial_prompt_length
        )
        stop_reason = "complete"  # Default for trial completion
        if len(response_ids) > max_response_tokens:
            stop_reason = "length"

        # Truncate to maximum allowed length
        response_ids = response_ids[:max_response_tokens]
        loss_mask = loss_mask[:max_response_tokens]
        if rollout_logprobs is not None:
            rollout_logprobs = rollout_logprobs[:max_response_tokens]

        return TerminalBenchAgentOutput(
            response_ids=response_ids,
            reward=reward,
            stop_reason=stop_reason,
            loss_mask=loss_mask,
            prompt_ids=prompt_ids,
            trajectory_id=trajectory_id,
            rollout_logprobs=rollout_logprobs,
            summarization_count=summarization_count,
        )
